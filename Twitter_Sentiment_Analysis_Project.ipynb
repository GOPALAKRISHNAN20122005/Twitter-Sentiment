{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4edb68e",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis using EDA and Deep Learning\n",
    "# Author: [Your Name]\n",
    "# Run this notebook in Google Colab with GPU runtime for best performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4016162",
   "metadata": {},
   "source": [
    "## Step 1: Install and Import Libraries\n",
    "!pip install -q wordcloud tensorflow nltk emoji seaborn matplotlib sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1520f3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string, numpy as np, pandas as pd, seaborn as sns, matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76b576a",
   "metadata": {},
   "source": [
    "## Step 2: Load Dataset\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "DATA_PATH = '/content/drive/MyDrive/twitter_airline_sentiment.csv'  # Update path\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f88f82a",
   "metadata": {},
   "source": [
    "## Step 3: EDA and Preprocessing\n",
    "df.drop_duplicates(inplace=True)\n",
    "df['text_length'] = df['text'].astype(str).apply(len)\n",
    "sns.countplot(x='airline_sentiment', data=df)\n",
    "plt.title('Sentiment Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5d0550",
   "metadata": {},
   "source": [
    "## Text cleaning\n",
    "nltk.download('stopwords'); nltk.download('punkt'); nltk.download('wordnet')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'@[A-Za-z0-9_]+', '', text)\n",
    "    text = re.sub(r'#[A-Za-z0-9_]+', '', text)\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "df['clean_text'] = df['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23754697",
   "metadata": {},
   "source": [
    "## Step 4: Tokenization and Train-Test Split\n",
    "le = LabelEncoder()\n",
    "df['label'] = le.fit_transform(df['airline_sentiment'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['clean_text'], df['label'], test_size=0.2, random_state=42)\n",
    "tokenizer = Tokenizer(num_words=20000, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "MAXLEN = 50\n",
    "X_train_seq = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=MAXLEN)\n",
    "X_test_seq = pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen=MAXLEN)\n",
    "y_train = tf.keras.utils.to_categorical(y_train)\n",
    "y_test = tf.keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33c02d4",
   "metadata": {},
   "source": [
    "## Step 5: Build and Train Model\n",
    "model = Sequential([\n",
    "    Embedding(20000, 128, input_length=MAXLEN),\n",
    "    Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(X_train_seq, y_train, validation_split=0.2, epochs=5, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b54d18f",
   "metadata": {},
   "source": [
    "## Step 6: Evaluation\n",
    "plt.plot(history.history['accuracy'], label='train')\n",
    "plt.plot(history.history['val_accuracy'], label='val')\n",
    "plt.legend(); plt.title('Accuracy vs Epochs'); plt.show()\n",
    "\n",
    "y_pred = np.argmax(model.predict(X_test_seq), axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "print(classification_report(y_true, y_pred, target_names=le.classes_))\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted'); plt.ylabel('True'); plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
